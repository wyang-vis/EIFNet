import torch as thimport torch.nn.functional as Ffrom torch import nnfrom .size_adapter import SizeAdapterimport torchfrom .arches import *from einops import rearrangeimport numbersclass ChannelAttention(nn.Module):    def __init__(self, in_planes, ratio=16):        super(ChannelAttention, self).__init__()        self.avg_pool = nn.AdaptiveAvgPool2d(1)        self.max_pool = nn.AdaptiveMaxPool2d(1)        self.fc1 = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)        self.relu1 = nn.ReLU(inplace=True)        self.fc2 = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)        self.sigmoid = nn.Sigmoid()    def forward(self, x):        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))        out = avg_out + max_out        out=self.sigmoid(out)        return outclass ChannelAttention_softmax(nn.Module):    def __init__(self, in_planes, ratio=16,L=32,M=2):        super(ChannelAttention_softmax, self).__init__()        self.avg_pool = nn.AdaptiveAvgPool2d(1)        self.max_pool = nn.AdaptiveMaxPool2d(1)        self.in_planes=in_planes        self.M=M        d = max(in_planes // ratio, L)        self.fc1=nn.Sequential(nn.Conv2d(in_planes,d,1,bias=False),                               nn.ReLU(inplace=True))        self.fc2=nn.Conv2d(d,in_planes*2,1,1,bias=False)        self.softmax=nn.Softmax(dim=1)        self.sigmoid = nn.Sigmoid()    def forward(self, x):        avg_out=self.avg_pool(x)        max_out=self.max_pool(x)        out = avg_out + max_out        out = self.fc1(out)        out_two = self.fc2(out)        batch_size = x.size(0)        out_two=out_two.reshape(batch_size,self.M,self.in_planes,-1)        out_two = self.softmax(out_two)        x_i, x_e = out_two[:, 0:1, :, :], out_two[:, 1:2, :, :]        x_i = x_i.reshape(batch_size, self.in_planes, 1, 1)        x_e = x_e.reshape(batch_size, self.in_planes, 1, 1)        return x_i, x_eclass SpatialAttention(nn.Module):    def __init__(self):        super(SpatialAttention, self).__init__()        self.conv1 = nn.Conv2d(2, 1,kernel_size=(3,3), padding=(1,1), bias=False)        self.sigmoid = nn.Sigmoid()    def forward(self, x):        avg_out = torch.mean(x, dim=1, keepdim=True)        max_out, _ = torch.max(x, dim=1, keepdim=True)        x = torch.cat([avg_out, max_out], dim=1)        x = self.conv1(x)        x = self.sigmoid(x)        return xclass SpatialAttention_softmax(nn.Module):    def __init__(self):        super(SpatialAttention_softmax, self).__init__()        self.conv1 = nn.Conv2d(2, 1,kernel_size=(3,3), padding=(1,1), bias=False)        self.conv2 = nn.Conv2d(2, 1,kernel_size=(3,3), padding=(1,1), bias=False)        self.softmax = nn.Softmax(dim=1)        self.sigmoid = nn.Sigmoid()    def forward(self, x):        avg_out = torch.mean(x, dim=1, keepdim=True)        max_out, _ = torch.max(x, dim=1, keepdim=True)        x = torch.cat([avg_out, max_out], dim=1)        x_i = self.conv1(x)        x_e = self.conv2(x)        x = torch.cat([x_i, x_e], dim=1)        x = self.softmax(x)        x_i, x_e = x[:, 0:1, :, :], x[:, 1:2, :, :]        return x_i,x_e# Channel Attention Layerclass CALayer(nn.Module):    def __init__(self, channel, reduction=16, bias=False):        super(CALayer, self).__init__()        # global average pooling: feature --> point        self.avg_pool = nn.AdaptiveAvgPool2d(1)        # feature channel downscale and upscale --> channel weight        self.conv_du = nn.Sequential(                nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=bias),                nn.ReLU(inplace=True),                nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=bias),                nn.Sigmoid()        )    def forward(self, x):        y = self.avg_pool(x)        y = self.conv_du(y)        return x * y## Channel Attention Block (CAB)class CAB(nn.Module):    def __init__(self, n_feat, kernel_size, reduction, bias, act):        super(CAB, self).__init__()        modules_body = []        modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))        modules_body.append(act)        modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))        self.CA = CALayer(n_feat, reduction, bias=bias)        self.body = nn.Sequential(*modules_body)    def forward(self, x):        res = self.body(x)        res = self.CA(res)        res += x        return resclass shallow_cell(nn.Module):    def __init__(self,inChannels):        super(shallow_cell, self).__init__()        self.n_feats = 64        act = nn.ReLU(inplace=True)        bias = False        reduction = 4        self.shallow_feat = nn.Sequential(conv(inChannels, self.n_feats, 3, bias=bias),                                           CAB(self.n_feats, 3, reduction, bias=bias, act=act))    def forward(self,x):        feat = self.shallow_feat(x)        return featclass EN_Block(nn.Module):    def __init__(self, in_planes, planes,kernel_size=3, reduction=4, bias=False):        super(EN_Block, self).__init__()        act = nn.ReLU(inplace=True)        self.down = DownSample(in_planes, planes)        self.encoder = [CAB(planes, kernel_size, reduction, bias=bias, act=act) for _ in range(2)]        self.encoder = nn.Sequential(*self.encoder)    def forward(self, x):        x = self.down(x)        x=self.encoder(x)        return xclass DE_Block(nn.Module):    def __init__(self, in_planes, planes, kernel_size=3, reduction=4, bias=False):        super(DE_Block, self).__init__()        act = nn.ReLU(inplace=True)        self.up=SkipUpSample(in_planes, planes)        self.decoder = [CAB(planes, kernel_size, reduction, bias=bias, act=act) for _ in range(2)]        self.decoder = nn.Sequential(*self.decoder)        self.skip_attn = CAB(planes, kernel_size, reduction, bias=bias, act=act)    def forward(self, x, skpCn):        x = self.up(x, self.skip_attn(skpCn))        x = self.decoder(x)        return x############################################################################ Layer Normclass Mlp(nn.Module):    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):        super().__init__()        out_features = out_features or in_features        hidden_features = hidden_features or in_features        self.fc1 = nn.Linear(in_features, hidden_features)        self.act = act_layer()        self.fc2 = nn.Linear(hidden_features, out_features)        self.drop = nn.Dropout(drop)    def forward(self, x):        x = self.fc1(x)        x = self.act(x)        x = self.drop(x)        x = self.fc2(x)        x = self.drop(x)        return xdef to_3d(x):    return rearrange(x, 'b c h w -> b (h w) c')def to_4d(x, h, w):    return rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)class BiasFree_LayerNorm(nn.Module):    def __init__(self, normalized_shape):        super(BiasFree_LayerNorm, self).__init__()        if isinstance(normalized_shape, numbers.Integral):            normalized_shape = (normalized_shape,)        normalized_shape = torch.Size(normalized_shape)        assert len(normalized_shape) == 1        self.weight = nn.Parameter(torch.ones(normalized_shape))        self.normalized_shape = normalized_shape    def forward(self, x):        sigma = x.var(-1, keepdim=True, unbiased=False)        return x / torch.sqrt(sigma + 1e-5) * self.weightclass WithBias_LayerNorm(nn.Module):    def __init__(self, normalized_shape):        super(WithBias_LayerNorm, self).__init__()        if isinstance(normalized_shape, numbers.Integral):            normalized_shape = (normalized_shape,)        normalized_shape = torch.Size(normalized_shape)        assert len(normalized_shape) == 1        self.weight = nn.Parameter(torch.ones(normalized_shape))        self.bias = nn.Parameter(torch.zeros(normalized_shape))        self.normalized_shape = normalized_shape    def forward(self, x):        mu = x.mean(-1, keepdim=True)        sigma = x.var(-1, keepdim=True, unbiased=False)        return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.biasclass LayerNorm(nn.Module):    def __init__(self, dim, LayerNorm_type):        super(LayerNorm, self).__init__()        if LayerNorm_type == 'BiasFree':            self.body = BiasFree_LayerNorm(dim)        else:            self.body = WithBias_LayerNorm(dim)    def forward(self, x):        h, w = x.shape[-2:]        return to_4d(self.body(to_3d(x)), h, w)class Recombine_Cross_Transformer(nn.Module):    def __init__(self, dim, num_heads, bias):        super(Recombine_Cross_Transformer, self).__init__()        self.num_heads = num_heads        self.temperature_inp1 = nn.Parameter(torch.ones(num_heads, 1, 1))        self.temperature_inp1 = nn.Parameter(torch.ones(num_heads, 1, 1))        self.q_inp1 = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)        self.k_inp1 = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)        self.v_inp1 = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)        self.q_inp2 = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)        self.k_inp2 = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)        self.v_inp2 = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)        self.project_out_1 = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)        self.project_out_2 = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)    def forward(self, inp1, inp2):        assert inp1.shape == inp2.shape, 'The shape of feature maps from image and event branch are not equal!'        b, c, h, w = inp1.shape        ###################img_attention###########################################        q_inp1 = self.q_inp1(inp1)  # image        k_inp1 = self.k_inp1(inp1)  # event        v_inp1 = self.v_inp1(inp1)  # event        q_inp1 = rearrange(q_inp1, 'b (head c) h w -> b head c (h w)', head=self.num_heads)        k_inp1 = rearrange(k_inp1, 'b (head c) h w -> b head c (h w)', head=self.num_heads)        v_inp1 = rearrange(v_inp1, 'b (head c) h w -> b head c (h w)', head=self.num_heads)        q_inp1 = torch.nn.functional.normalize(q_inp1, dim=-1)        k_inp1 = torch.nn.functional.normalize(k_inp1, dim=-1)        v_inp1 = torch.nn.functional.normalize(v_inp1, dim=-1)        ###################event_attention###########################################        q_inp2 = self.q_inp2(inp2)  # image        k_inp2 = self.k_inp2(inp2)  # event        v_inp2 = self.v_inp2(inp2)  # event        q_inp2 = rearrange(q_inp2, 'b (head c) h w -> b head c (h w)', head=self.num_heads)        k_inp2 = rearrange(k_inp2, 'b (head c) h w -> b head c (h w)', head=self.num_heads)        v_inp2 = rearrange(v_inp2, 'b (head c) h w -> b head c (h w)', head=self.num_heads)        q_inp2 = torch.nn.functional.normalize(q_inp2, dim=-1)        k_inp2 = torch.nn.functional.normalize(k_inp2, dim=-1)        v_inp2 = torch.nn.functional.normalize(v_inp2, dim=-1)        ###################cross_attention###########################################        attn_inp1 = (q_inp1 @ k_inp2.transpose(-2, -1)) * self.temperature_inp1        attn_inp1 = attn_inp1.softmax(dim=-1)        out_inp1=(attn_inp1 @ v_inp2)        out_inp1 = rearrange(out_inp1, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)        out_inp1 = self.project_out_1(out_inp1)        attn_inp2 = (q_inp2 @ k_inp1.transpose(-2, -1)) * self.temperature_inp1        attn_inp2 = attn_inp2.softmax(dim=-1)        out_inp2=(attn_inp2 @ v_inp1)        out_inp2 = rearrange(out_inp2, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)        out_inp2 = self.project_out_2(out_inp2)        return out_inp1,out_inp2class Divide_Cross_Transformer(nn.Module):    def __init__(self, dim, num_heads, bias):        super(Divide_Cross_Transformer, self).__init__()        self.num_heads = num_heads        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))        self.q = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)        self.k = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)        self.v = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)    def forward(self, inp1, inp2):        assert inp1.shape == inp2.shape, 'The shape of feature maps from image and event branch are not equal!'        b, c, h, w = inp1.shape        q = self.q(inp2)  # image        k = self.k(inp1)  # event        v = self.v(inp1)  # event        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)        q = torch.nn.functional.normalize(q, dim=-1)        k = torch.nn.functional.normalize(k, dim=-1)        attn = (q @ k.transpose(-2, -1)) * self.temperature        attn = attn.softmax(dim=-1)        return attn,v